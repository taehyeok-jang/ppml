{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43029768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 21:32:04.081210: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-10 21:32:04.168407: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-10 21:32:04.193869: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-10 21:32:06.105441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# pylint: skip-file\n",
    "# pyformat: disable\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import shutil\n",
    "from typing import Callable\n",
    "import json\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jn\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf  # For data augmentation.\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import app, flags\n",
    "\n",
    "import objax\n",
    "from objax.jaxboard import SummaryWriter, Summary\n",
    "from objax.util import EasyDict\n",
    "from objax.zoo import convnet, wide_resnet\n",
    "\n",
    "from dataset import DataSet\n",
    "\n",
    "\n",
    "############################# from MODEL ZOO\n",
    "import torch\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f7193d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(x, shift: int, mirror=True):\n",
    "    \"\"\"\n",
    "    Augmentation function used in training the model.\n",
    "    \"\"\"\n",
    "    y = x['image']\n",
    "    if mirror:\n",
    "        y = tf.image.random_flip_left_right(y)\n",
    "    y = tf.pad(y, [[shift] * 2, [shift] * 2, [0] * 2], mode='REFLECT')\n",
    "    y = tf.image.random_crop(y, tf.shape(x['image']))\n",
    "    return dict(image=y, label=x['label'])\n",
    "\n",
    "\n",
    "class TrainLoop(objax.Module):\n",
    "    \"\"\"\n",
    "    Training loop for general machine learning models.\n",
    "    Based on the training loop from the objax CIFAR10 example code.\n",
    "    \"\"\"\n",
    "    predict: Callable\n",
    "    train_op: Callable\n",
    "\n",
    "#     def __init__(self, nclass: int, **kwargs):\n",
    "#         self.nclass = nclass\n",
    "#         self.params = EasyDict(kwargs)\n",
    "\n",
    "#     def train_step(self, summary: Summary, data: dict, progress: np.ndarray):\n",
    "#         kv = self.train_op(progress, data['image'].numpy(), data['label'].numpy())\n",
    "#         for k, v in kv.items():\n",
    "#             if jn.isnan(v):\n",
    "#                 raise ValueError('NaN, try reducing learning rate', k)\n",
    "#             if isinstance(v, jn.ndarray) and v.ndim > 0:\n",
    "#                 v = v.item()  # Converts array to a scalar if it's a single value array\n",
    "                \n",
    "#             if summary is not None:\n",
    "#                 summary.scalar(k, float(v))\n",
    "\n",
    "    def train(self, num_train_epochs: int, train_size: int, train: DataSet, test: DataSet, logdir: str, save_steps=100, patience=None):\n",
    "        \"\"\"\n",
    "        Completely standard training. Nothing interesting to see here.\n",
    "        \"\"\"\n",
    "        checkpoint = objax.io.Checkpoint(logdir, keep_ckpts=20, makedir=True)\n",
    "        start_epoch, last_ckpt = checkpoint.restore(self.vars())\n",
    "        train_iter = iter(train)\n",
    "        progress = np.zeros(jax.local_device_count(), 'f')  # for multi-GPU\n",
    "\n",
    "        best_acc = 0\n",
    "        best_acc_epoch = -1\n",
    "\n",
    "        with SummaryWriter(os.path.join(logdir, 'tb')) as tensorboard:\n",
    "            for epoch in range(start_epoch, num_train_epochs):\n",
    "                # Train\n",
    "                summary = Summary()\n",
    "                loop = range(0, train_size, self.params.batch)\n",
    "                for step in loop:\n",
    "                    progress[:] = (step + (epoch * train_size)) / (num_train_epochs * train_size)\n",
    "                    \n",
    "                    next_iter = next(train_iter)\n",
    "                    print(\"next_iter: \", next_iter.shape)\n",
    "                    \n",
    "                    self.train_step(summary, next_iter, progress)\n",
    "\n",
    "                Eval\n",
    "                accuracy, total = 0, 0\n",
    "                if epoch%FLAGS.eval_steps == 0 and test is not None:\n",
    "                    for data in test:\n",
    "                        total += data['image'].shape[0]\n",
    "                        preds = np.argmax(self.predict(data['image'].numpy()), axis=1)\n",
    "                        accuracy += (preds == data['label'].numpy()).sum()\n",
    "                    accuracy /= total\n",
    "                    summary.scalar('eval/accuracy', 100 * accuracy)\n",
    "                    tensorboard.write(summary, step=(epoch + 1) * train_size)\n",
    "                    print('Epoch %04d  Loss %.2f  Accuracy %.2f' % (epoch + 1, summary['losses/xe'](),\n",
    "                                                                    summary['eval/accuracy']()))\n",
    "\n",
    "                    if summary['eval/accuracy']() > best_acc:\n",
    "                        best_acc = summary['eval/accuracy']()\n",
    "                        best_acc_epoch = epoch\n",
    "                    elif patience is not None and epoch > best_acc_epoch + patience:\n",
    "                        print(\"early stopping!\")\n",
    "                        checkpoint.save(self.vars(), epoch + 1)\n",
    "                        return\n",
    "\n",
    "                else:\n",
    "                    print('Epoch %04d  Loss %.2f  Accuracy --' % (epoch + 1, summary['losses/xe']()))\n",
    "\n",
    "                if epoch%save_steps == save_steps-1:\n",
    "                    checkpoint.save(self.vars(), epoch + 1)\n",
    "\n",
    "\n",
    "# We inherit from the training loop and define predict and train_op.\n",
    "class MemModule(TrainLoop):\n",
    "    def __init__(self, model: Callable, nclass: int, mnist=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Completely standard training. Nothing interesting to see here.\n",
    "        \"\"\"\n",
    "        super().__init__(nclass, **kwargs)\n",
    "        self.model = model(1 if mnist else 3, nclass)\n",
    "        self.opt = objax.optimizer.Momentum(self.model.vars())\n",
    "        self.model_ema = objax.optimizer.ExponentialMovingAverageModule(self.model, momentum=0.999, debias=True)\n",
    "\n",
    "        @objax.Function.with_vars(self.model.vars())\n",
    "#         def loss(x, label):\n",
    "#             logit = self.model(x, training=True)\n",
    "#             loss_wd = 0.5 * sum((v.value ** 2).sum() for k, v in self.model.vars().items() if k.endswith('.w'))\n",
    "#             loss_xe = objax.functional.loss.cross_entropy_logits(logit, label).mean()\n",
    "#             return loss_xe + loss_wd * self.params.weight_decay, {'losses/xe': loss_xe, 'losses/wd': loss_wd}\n",
    "\n",
    "        gv = objax.GradValues(loss, self.model.vars())\n",
    "        self.gv = gv\n",
    "\n",
    "        @objax.Function.with_vars(self.vars())\n",
    "#         def train_op(progress, x, y):\n",
    "#             g, v = gv(x, y)\n",
    "#             lr = self.params.lr * jn.cos(progress * (7 * jn.pi) / (2 * 8))\n",
    "#             lr = lr * jn.clip(progress*100,0,1)\n",
    "#             self.opt(lr, g)\n",
    "#             self.model_ema.update_ema()\n",
    "#             return {'monitors/lr': lr, **v[1]}\n",
    "\n",
    "        self.predict = objax.Jit(objax.nn.Sequential([objax.ForceArgs(self.model_ema, training=False)]))\n",
    "\n",
    "        self.train_op = objax.Jit(train_op)\n",
    "\n",
    "\n",
    "# def network(arch: str):\n",
    "#     if arch == 'cnn32-3-max':\n",
    "#         return functools.partial(convnet.ConvNet, scales=3, filters=32, filters_max=1024,\n",
    "#                                  pooling=objax.functional.max_pool_2d)\n",
    "#     elif arch == 'cnn32-3-mean':\n",
    "#         return functools.partial(convnet.ConvNet, scales=3, filters=32, filters_max=1024,\n",
    "#                                  pooling=objax.functional.average_pool_2d)\n",
    "#     elif arch == 'cnn64-3-max':\n",
    "#         return functools.partial(convnet.ConvNet, scales=3, filters=64, filters_max=1024,\n",
    "#                                  pooling=objax.functional.max_pool_2d)\n",
    "#     elif arch == 'cnn64-3-mean':\n",
    "#         return functools.partial(convnet.ConvNet, scales=3, filters=64, filters_max=1024,\n",
    "#                                  pooling=objax.functional.average_pool_2d)\n",
    "#     elif arch == 'wrn28-1':\n",
    "#         return functools.partial(wide_resnet.WideResNet, depth=28, width=1)\n",
    "#     elif arch == 'wrn28-2':\n",
    "#         return functools.partial(wide_resnet.WideResNet, depth=28, width=2)\n",
    "#     elif arch == 'wrn28-10':\n",
    "#         return functools.partial(wide_resnet.WideResNet, depth=28, width=10)\n",
    "#     raise ValueError('Architecture not recognized', arch)\n",
    "    \n",
    "def get_data(seed):\n",
    "    \"\"\"\n",
    "    This is the function to generate subsets of the data for training models.\n",
    "\n",
    "    First, we get the training dataset either from the numpy cache\n",
    "    or otherwise we load it from tensorflow datasets.\n",
    "\n",
    "    Then, we compute the subset. This works in one of two ways.\n",
    "\n",
    "    1. If we have a seed, then we just randomly choose examples based on\n",
    "       a prng with that seed, keeping FLAGS.pkeep fraction of the data.\n",
    "\n",
    "    2. Otherwise, if we have an experiment ID, then we do something fancier.\n",
    "       If we run each experiment independently then even after a lot of trials\n",
    "       there will still probably be some examples that were always included\n",
    "       or always excluded. So instead, with experiment IDs, we guarantee that\n",
    "       after FLAGS.num_experiments are done, each example is seen exactly half\n",
    "       of the time in train, and half of the time not in train.\n",
    "\n",
    "    \"\"\"\n",
    "#     DATA_DIR = os.path.join(os.environ['HOME'], 'TFDS')\n",
    "    \n",
    "    if os.path.exists(os.path.join(FLAGS.logdir, \"x_train.npy\")):\n",
    "        inputs = np.load(os.path.join(FLAGS.logdir, \"x_train.npy\"))\n",
    "        labels = np.load(os.path.join(FLAGS.logdir, \"y_train.npy\"))\n",
    "    else:\n",
    "        print(\"First time, creating dataset\")\n",
    "\n",
    "        ### \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # https://huggingface.co/datasets/ILSVRC/imagenet-1k\n",
    "        data_dir = '/serenity/scratch/psml/repo/psml/data/ILSVRC2012'\n",
    "        imagenet_data = datasets.ImageNet(root=data_dir, split='val', transform=transform)\n",
    "        \n",
    "        data_loader = torch.utils.data.DataLoader(imagenet_data, batch_size=256, shuffle=False, num_workers=4)\n",
    "\n",
    "        inputs_ = [] \n",
    "        labels_ = []\n",
    "        iter = 0\n",
    "        for images__, labels__ in data_loader:     \n",
    "            print(iter)\n",
    "            iter += 1\n",
    "            inputs_.append(images__)\n",
    "            labels_.append(labels__)\n",
    "\n",
    "        inputs = np.concatenate(inputs_, axis=0)\n",
    "        labels = np.concatenate(labels_, axis=0)\n",
    "        ### \n",
    "        \n",
    "        np.save(os.path.join(FLAGS.logdir, \"x_train.npy\"),inputs)\n",
    "        np.save(os.path.join(FLAGS.logdir, \"y_train.npy\"),labels)\n",
    "\n",
    "    nclass = np.max(labels)+1\n",
    "\n",
    "    # TODO \n",
    "    # 1. split inputs and label to train / test datasets with the ratio 80:20.\n",
    "    # 2. \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    if FLAGS.num_experiments is not None:\n",
    "        np.random.seed(0)\n",
    "        keep = np.random.uniform(0,1,size=(FLAGS.num_experiments, FLAGS.dataset_size))\n",
    "        order = keep.argsort(0)\n",
    "        keep = order < int(FLAGS.pkeep * FLAGS.num_experiments)\n",
    "        keep = np.array(keep[FLAGS.expid], dtype=bool)\n",
    "    else:\n",
    "        keep = np.random.uniform(0, 1, size=FLAGS.dataset_size) <= FLAGS.pkeep\n",
    "\n",
    "    if FLAGS.only_subset is not None:\n",
    "        keep[FLAGS.only_subset:] = 0\n",
    "        \n",
    "    \n",
    "\n",
    "    xs = inputs[keep]\n",
    "    ys = labels[keep]\n",
    "\n",
    "    if FLAGS.augment == 'weak':\n",
    "        aug = lambda x: augment(x, 4)\n",
    "    elif FLAGS.augment == 'mirror':\n",
    "        aug = lambda x: augment(x, 0)\n",
    "    elif FLAGS.augment == 'none':\n",
    "        aug = lambda x: augment(x, 0, mirror=False)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    train = DataSet.from_arrays(xs, ys,\n",
    "                                augment_fn=aug)\n",
    "    test = DataSet.from_tfds(tfds.load(name=FLAGS.dataset, split='test', data_dir=DATA_DIR), xs.shape[1:])\n",
    "    train = train.cache().shuffle(8192).repeat().parse().augment().batch(FLAGS.batch)\n",
    "    train = train.nchw().one_hot(nclass).prefetch(16)\n",
    "    test = test.cache().parse().batch(FLAGS.batch).nchw().prefetch(16)\n",
    "\n",
    "    return train, test, xs, ys, keep, nclass\n",
    "\n",
    "def main(argv):\n",
    "    del argv\n",
    "    tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "\n",
    "    seed = FLAGS.seed\n",
    "    if seed is None:\n",
    "        import time\n",
    "        seed = np.random.randint(0, 1000000000)\n",
    "        seed ^= int(time.time())\n",
    "\n",
    "    args = EasyDict(arch=FLAGS.arch,\n",
    "                    lr=FLAGS.lr,\n",
    "                    batch=FLAGS.batch,\n",
    "                    weight_decay=FLAGS.weight_decay,\n",
    "                    augment=FLAGS.augment,\n",
    "                    seed=seed)\n",
    "\n",
    "\n",
    "    if FLAGS.tunename:\n",
    "        logdir = '_'.join(sorted('%s=%s' % k for k in args.items()))\n",
    "    elif FLAGS.expid is not None:\n",
    "        logdir = \"experiment-%d_%d\"%(FLAGS.expid,FLAGS.num_experiments)\n",
    "    else:\n",
    "        logdir = \"experiment-\"+str(seed)\n",
    "    logdir = os.path.join(FLAGS.logdir, logdir)\n",
    "\n",
    "    if os.path.exists(os.path.join(logdir, \"ckpt\", \"%010d.npz\"%FLAGS.epochs)):\n",
    "        print(f\"run {FLAGS.expid} already completed.\")\n",
    "        return\n",
    "    else:\n",
    "        if os.path.exists(logdir):\n",
    "            print(f\"deleting run {FLAGS.expid} that did not complete.\")\n",
    "            shutil.rmtree(logdir)\n",
    "\n",
    "    print(f\"starting run {FLAGS.expid}.\")\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    train, test, xs, ys, keep, nclass = get_data(seed)\n",
    "\n",
    "    # Define the network and train_it\n",
    "    tm = MemModule(network(FLAGS.arch), nclass=nclass,\n",
    "                   mnist=FLAGS.dataset == 'mnist',\n",
    "                   epochs=FLAGS.epochs,\n",
    "                   expid=FLAGS.expid,\n",
    "                   num_experiments=FLAGS.num_experiments,\n",
    "                   pkeep=FLAGS.pkeep,\n",
    "                   save_steps=FLAGS.save_steps,\n",
    "                   only_subset=FLAGS.only_subset,\n",
    "                   **args\n",
    "    )\n",
    "\n",
    "    r = {}\n",
    "    r.update(tm.params)\n",
    "\n",
    "    open(os.path.join(logdir,'hparams.json'),\"w\").write(json.dumps(tm.params))\n",
    "    np.save(os.path.join(logdir,'keep.npy'), keep)\n",
    "\n",
    "    tm.train(FLAGS.epochs, len(xs), train, test, logdir,\n",
    "             save_steps=FLAGS.save_steps, patience=FLAGS.patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8633b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nif __name__ == '__main__':\\n    flags.DEFINE_string('arch', 'cnn32-3-mean', 'Model architecture.')\\n    flags.DEFINE_float('lr', 0.1, 'Learning rate.')\\n    flags.DEFINE_string('dataset', 'cifar10', 'Dataset.')\\n    flags.DEFINE_float('weight_decay', 0.0005, 'Weight decay ratio.')\\n    flags.DEFINE_integer('batch', 256, 'Batch size')\\n    flags.DEFINE_integer('epochs', 501, 'Training duration in number of epochs.')\\n    flags.DEFINE_string('logdir', 'experiments', 'Directory where to save checkpoints and tensorboard data.')\\n    flags.DEFINE_integer('seed', None, 'Training seed.')\\n    flags.DEFINE_float('pkeep', .5, 'Probability to keep examples.')\\n    flags.DEFINE_integer('expid', None, 'Experiment ID')\\n    flags.DEFINE_integer('num_experiments', None, 'Number of experiments')\\n    flags.DEFINE_string('augment', 'weak', 'Strong or weak augmentation')\\n    flags.DEFINE_integer('only_subset', None, 'Only train on a subset of images.')\\n    flags.DEFINE_integer('dataset_size', 50000, 'number of examples to keep.')\\n    flags.DEFINE_integer('eval_steps', 1, 'how often to get eval accuracy.')\\n    flags.DEFINE_integer('abort_after_epoch', None, 'stop trainin early at an epoch')\\n    flags.DEFINE_integer('save_steps', 10, 'how often to get save model.')\\n    flags.DEFINE_integer('patience', None, 'Early stopping after this many epochs without progress')\\n    flags.DEFINE_bool('tunename', False, 'Use tune name?')\\n    \\n    ### override \\n    # https://github.com/tensorflow/privacy/tree/d965556ebb67bd62626830339478e9ebab7ab9bd/research/mi_lira_2021/scripts  \\n#     CUDA_VISIBLE_DEVICES='0' python3 -u train.py --dataset=cifar10 --epochs=100 --save_steps=20 --arch wrn28-2 --num_experiments 16 --expid 0 --logdir exp/cifar10 &> logs/log_0\\n\\n    flags.DEFINE_string('dataset', 'cifar10', 'Dataset.')\\n    flags.DEFINE_string('arch', 'wrn28-2', 'Model architecture.')\\n\\n    flags.DEFINE_integer('epochs', 20, 'Training duration in number of epochs.')\\n    flags.DEFINE_integer('save_steps', 5, 'how often to get save model.')\\n    flags.DEFINE_integer('num_experiments', 16, 'Number of experiments')\\n    flags.DEFINE_integer('expid', 3, 'Experiment ID')\\n    flags.DEFINE_string('logdir', 'experiments', 'Directory where to save checkpoints and tensorboard data.')\\n        \\n    app.run(main)\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "if __name__ == '__main__':\n",
    "    flags.DEFINE_string('arch', 'cnn32-3-mean', 'Model architecture.')\n",
    "    flags.DEFINE_float('lr', 0.1, 'Learning rate.')\n",
    "    flags.DEFINE_string('dataset', 'cifar10', 'Dataset.')\n",
    "    flags.DEFINE_float('weight_decay', 0.0005, 'Weight decay ratio.')\n",
    "    flags.DEFINE_integer('batch', 256, 'Batch size')\n",
    "    flags.DEFINE_integer('epochs', 501, 'Training duration in number of epochs.')\n",
    "    flags.DEFINE_string('logdir', 'experiments', 'Directory where to save checkpoints and tensorboard data.')\n",
    "    flags.DEFINE_integer('seed', None, 'Training seed.')\n",
    "    flags.DEFINE_float('pkeep', .5, 'Probability to keep examples.')\n",
    "    flags.DEFINE_integer('expid', None, 'Experiment ID')\n",
    "    flags.DEFINE_integer('num_experiments', None, 'Number of experiments')\n",
    "    flags.DEFINE_string('augment', 'weak', 'Strong or weak augmentation')\n",
    "    flags.DEFINE_integer('only_subset', None, 'Only train on a subset of images.')\n",
    "    flags.DEFINE_integer('dataset_size', 50000, 'number of examples to keep.')\n",
    "    flags.DEFINE_integer('eval_steps', 1, 'how often to get eval accuracy.')\n",
    "    flags.DEFINE_integer('abort_after_epoch', None, 'stop trainin early at an epoch')\n",
    "    flags.DEFINE_integer('save_steps', 10, 'how often to get save model.')\n",
    "    flags.DEFINE_integer('patience', None, 'Early stopping after this many epochs without progress')\n",
    "    flags.DEFINE_bool('tunename', False, 'Use tune name?')\n",
    "    \n",
    "    ### override \n",
    "    # https://github.com/tensorflow/privacy/tree/d965556ebb67bd62626830339478e9ebab7ab9bd/research/mi_lira_2021/scripts  \n",
    "#     CUDA_VISIBLE_DEVICES='0' python3 -u train.py --dataset=cifar10 --epochs=100 --save_steps=20 --arch wrn28-2 --num_experiments 16 --expid 0 --logdir exp/cifar10 &> logs/log_0\n",
    "\n",
    "    flags.DEFINE_string('dataset', 'cifar10', 'Dataset.')\n",
    "    flags.DEFINE_string('arch', 'wrn28-2', 'Model architecture.')\n",
    "\n",
    "    flags.DEFINE_integer('epochs', 20, 'Training duration in number of epochs.')\n",
    "    flags.DEFINE_integer('save_steps', 5, 'how often to get save model.')\n",
    "    flags.DEFINE_integer('num_experiments', 16, 'Number of experiments')\n",
    "    flags.DEFINE_integer('expid', 3, 'Experiment ID')\n",
    "    flags.DEFINE_string('logdir', 'experiments', 'Directory where to save checkpoints and tensorboard data.')\n",
    "        \n",
    "    app.run(main)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ca089b",
   "metadata": {},
   "source": [
    "# import imagenet using tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f20d452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downsampled_imagenet\n",
      "imagenet2012\n",
      "imagenet2012_corrupted\n",
      "imagenet2012_fewshot\n",
      "imagenet2012_multilabel\n",
      "imagenet2012_real\n",
      "imagenet2012_subset\n",
      "imagenet_a\n",
      "imagenet_lt\n",
      "imagenet_pi\n",
      "imagenet_r\n",
      "imagenet_resized\n",
      "imagenet_sketch\n",
      "imagenet_v2\n",
      "imagenette\n",
      "huggingface:imagenet-1k\n",
      "huggingface:imagenet_sketch\n"
     ]
    }
   ],
   "source": [
    "for ds in tfds.list_builders(): \n",
    "    if 'imagenet' in ds:\n",
    "        print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41a8eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.environ['HOME'], 'TFDS')\n",
    "\n",
    "data = tfds.as_numpy(tfds.load(name='cifar10', batch_size=-1, data_dir=DATA_DIR))\n",
    "inputs = data['train']['image']\n",
    "labels = data['train']['label']\n",
    "\n",
    "inputs = (inputs/127.5)-1\n",
    "\n",
    "nclass = np.max(labels)+1\n",
    "\n",
    "seed = 128 # customization \n",
    "num_experiments = 16\n",
    "dataset_size = 50000\n",
    "pkeep = 0.5\n",
    "expid = 1\n",
    "only_subset = None\n",
    "batch = 32\n",
    "\n",
    "np.random.seed(seed)\n",
    "if num_experiments is not None:\n",
    "    np.random.seed(0)\n",
    "    keep = np.random.uniform(0,1,size=(num_experiments, dataset_size))\n",
    "    order = keep.argsort(0)\n",
    "    keep = order < int(pkeep * num_experiments)\n",
    "    keep = np.array(keep[expid], dtype=bool)\n",
    "else:\n",
    "    keep = np.random.uniform(0, 1, size=dataset_size) <= pkeep\n",
    "\n",
    "if only_subset is not None:\n",
    "    keep[only_subset:] = 0\n",
    "\n",
    "xs = inputs[keep]\n",
    "ys = labels[keep]\n",
    "\n",
    "# if FLAGS.augment == 'weak':\n",
    "#     aug = lambda x: augment(x, 4)\n",
    "# elif FLAGS.augment == 'mirror':\n",
    "#     aug = lambda x: augment(x, 0)\n",
    "# elif FLAGS.augment == 'none':\n",
    "#     aug = lambda x: augment(x, 0, mirror=False)\n",
    "# else:\n",
    "#     raise\n",
    "\n",
    "aug = lambda x: augment(x, 0, mirror=False)\n",
    "\n",
    "train = DataSet.from_arrays(xs, ys,\n",
    "                            augment_fn=aug)\n",
    "test = DataSet.from_tfds(tfds.load(name='cifar10', split='test', data_dir=DATA_DIR), xs.shape[1:])\n",
    "train = train.cache().shuffle(8192).repeat().parse().augment().batch(batch)\n",
    "train = train.nchw().one_hot(nclass).prefetch(16)\n",
    "test = test.cache().parse().batch(batch).nchw().prefetch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efe29bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25046, 32, 32, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "042aaa47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.59215686,  0.67843137,  0.83529412],\n",
       "        [ 0.49803922,  0.62352941,  0.77254902],\n",
       "        [ 0.39607843,  0.56862745,  0.75686275],\n",
       "        ...,\n",
       "        [-0.00392157,  0.34901961,  0.67058824],\n",
       "        [-0.01176471,  0.34117647,  0.6627451 ],\n",
       "        [-0.02745098,  0.33333333,  0.65490196]],\n",
       "\n",
       "       [[ 0.60784314,  0.67843137,  0.80392157],\n",
       "        [ 0.45882353,  0.56078431,  0.67058824],\n",
       "        [ 0.41176471,  0.54509804,  0.67843137],\n",
       "        ...,\n",
       "        [ 0.03529412,  0.39607843,  0.71764706],\n",
       "        [ 0.01960784,  0.38039216,  0.71764706],\n",
       "        [ 0.01176471,  0.37254902,  0.70196078]],\n",
       "\n",
       "       [[ 0.51372549,  0.56862745,  0.67058824],\n",
       "        [ 0.10588235,  0.18431373,  0.24705882],\n",
       "        [-0.02745098,  0.0745098 ,  0.1372549 ],\n",
       "        ...,\n",
       "        [ 0.06666667,  0.39607843,  0.70980392],\n",
       "        [ 0.05098039,  0.38823529,  0.70980392],\n",
       "        [ 0.03529412,  0.38039216,  0.70196078]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.68627451, -0.63137255, -0.56078431],\n",
       "        [-0.74117647, -0.70980392, -0.67058824],\n",
       "        [-0.75686275, -0.7254902 , -0.67843137],\n",
       "        ...,\n",
       "        [-0.42745098, -0.22352941,  0.03529412],\n",
       "        [-0.49803922, -0.28627451, -0.01176471],\n",
       "        [-0.45882353, -0.23921569,  0.04313725]],\n",
       "\n",
       "       [[-0.70980392, -0.65490196, -0.58431373],\n",
       "        [-0.75686275, -0.73333333, -0.68627451],\n",
       "        [-0.76470588, -0.73333333, -0.68627451],\n",
       "        ...,\n",
       "        [-0.43529412, -0.23137255,  0.03529412],\n",
       "        [-0.49803922, -0.27843137, -0.00392157],\n",
       "        [-0.46666667, -0.24705882,  0.03529412]],\n",
       "\n",
       "       [[-0.73333333, -0.67843137, -0.60784314],\n",
       "        [-0.77254902, -0.74901961, -0.70196078],\n",
       "        [-0.78039216, -0.74901961, -0.70196078],\n",
       "        ...,\n",
       "        [-0.46666667, -0.2627451 , -0.00392157],\n",
       "        [-0.51372549, -0.30196078, -0.03529412],\n",
       "        [-0.50588235, -0.28627451, -0.01176471]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc5f1424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25046,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00f6b214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "image\n",
      "label\n"
     ]
    }
   ],
   "source": [
    "for k,v in data['train'].items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "357ad18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['train']['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2579374b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['id'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f53d1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'train_16399', b'train_01680', b'train_47917', ...,\n",
       "       b'train_05073', b'train_39601', b'train_32842'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d52932b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e55e703d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[143,  96,  70],\n",
       "         [141,  96,  72],\n",
       "         [135,  93,  72],\n",
       "         ...,\n",
       "         [ 96,  37,  19],\n",
       "         [105,  42,  18],\n",
       "         [104,  38,  20]],\n",
       "\n",
       "        [[128,  98,  92],\n",
       "         [146, 118, 112],\n",
       "         [170, 145, 138],\n",
       "         ...,\n",
       "         [108,  45,  26],\n",
       "         [112,  44,  24],\n",
       "         [112,  41,  22]],\n",
       "\n",
       "        [[ 93,  69,  75],\n",
       "         [118,  96, 101],\n",
       "         [179, 160, 162],\n",
       "         ...,\n",
       "         [128,  68,  47],\n",
       "         [125,  61,  42],\n",
       "         [122,  59,  39]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[187, 150, 123],\n",
       "         [184, 148, 123],\n",
       "         [179, 142, 121],\n",
       "         ...,\n",
       "         [198, 163, 132],\n",
       "         [201, 166, 135],\n",
       "         [207, 174, 143]],\n",
       "\n",
       "        [[187, 150, 117],\n",
       "         [181, 143, 115],\n",
       "         [175, 136, 113],\n",
       "         ...,\n",
       "         [201, 164, 132],\n",
       "         [205, 168, 135],\n",
       "         [207, 171, 139]],\n",
       "\n",
       "        [[195, 161, 126],\n",
       "         [187, 153, 123],\n",
       "         [186, 151, 128],\n",
       "         ...,\n",
       "         [212, 177, 147],\n",
       "         [219, 185, 155],\n",
       "         [221, 187, 157]]],\n",
       "\n",
       "\n",
       "       [[[203, 214, 234],\n",
       "         [191, 207, 226],\n",
       "         [178, 200, 224],\n",
       "         ...,\n",
       "         [127, 172, 213],\n",
       "         [126, 171, 212],\n",
       "         [124, 170, 211]],\n",
       "\n",
       "        [[205, 214, 230],\n",
       "         [186, 199, 213],\n",
       "         [180, 197, 214],\n",
       "         ...,\n",
       "         [132, 178, 219],\n",
       "         [130, 176, 219],\n",
       "         [129, 175, 217]],\n",
       "\n",
       "        [[193, 200, 213],\n",
       "         [141, 151, 159],\n",
       "         [124, 137, 145],\n",
       "         ...,\n",
       "         [136, 178, 218],\n",
       "         [134, 177, 218],\n",
       "         [132, 176, 217]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 40,  47,  56],\n",
       "         [ 33,  37,  42],\n",
       "         [ 31,  35,  41],\n",
       "         ...,\n",
       "         [ 73,  99, 132],\n",
       "         [ 64,  91, 126],\n",
       "         [ 69,  97, 133]],\n",
       "\n",
       "        [[ 37,  44,  53],\n",
       "         [ 31,  34,  40],\n",
       "         [ 30,  34,  40],\n",
       "         ...,\n",
       "         [ 72,  98, 132],\n",
       "         [ 64,  92, 127],\n",
       "         [ 68,  96, 132]],\n",
       "\n",
       "        [[ 34,  41,  50],\n",
       "         [ 29,  32,  38],\n",
       "         [ 28,  32,  38],\n",
       "         ...,\n",
       "         [ 68,  94, 127],\n",
       "         [ 62,  89, 123],\n",
       "         [ 63,  91, 126]]],\n",
       "\n",
       "\n",
       "       [[[106, 103, 104],\n",
       "         [103,  97,  99],\n",
       "         [102,  93,  96],\n",
       "         ...,\n",
       "         [135, 126, 129],\n",
       "         [139, 130, 133],\n",
       "         [131, 122, 125]],\n",
       "\n",
       "        [[106, 104, 105],\n",
       "         [105,  99, 101],\n",
       "         [115, 106, 109],\n",
       "         ...,\n",
       "         [137, 129, 132],\n",
       "         [135, 126, 129],\n",
       "         [124, 115, 118]],\n",
       "\n",
       "        [[108, 105, 106],\n",
       "         [117, 111, 113],\n",
       "         [123, 114, 117],\n",
       "         ...,\n",
       "         [132, 123, 126],\n",
       "         [126, 117, 120],\n",
       "         [121, 112, 115]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[136, 163, 117],\n",
       "         [135, 162, 115],\n",
       "         [139, 166, 117],\n",
       "         ...,\n",
       "         [140, 144, 111],\n",
       "         [133, 135, 105],\n",
       "         [126, 125,  98]],\n",
       "\n",
       "        [[130, 150, 104],\n",
       "         [131, 150, 107],\n",
       "         [131, 150, 109],\n",
       "         ...,\n",
       "         [141, 150, 112],\n",
       "         [144, 152, 115],\n",
       "         [140, 144, 111]],\n",
       "\n",
       "        [[139, 151, 108],\n",
       "         [133, 144, 103],\n",
       "         [145, 156, 116],\n",
       "         ...,\n",
       "         [129, 137, 100],\n",
       "         [138, 144, 110],\n",
       "         [134, 136, 106]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[200, 223, 241],\n",
       "         [195, 225, 247],\n",
       "         [192, 226, 249],\n",
       "         ...,\n",
       "         [198, 227, 247],\n",
       "         [197, 226, 245],\n",
       "         [191, 223, 242]],\n",
       "\n",
       "        [[193, 225, 245],\n",
       "         [191, 228, 251],\n",
       "         [191, 230, 253],\n",
       "         ...,\n",
       "         [197, 231, 251],\n",
       "         [194, 228, 249],\n",
       "         [189, 226, 247]],\n",
       "\n",
       "        [[186, 224, 245],\n",
       "         [183, 225, 248],\n",
       "         [181, 223, 244],\n",
       "         ...,\n",
       "         [195, 234, 253],\n",
       "         [193, 232, 252],\n",
       "         [191, 231, 250]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[195, 203, 204],\n",
       "         [192, 200, 200],\n",
       "         [195, 204, 202],\n",
       "         ...,\n",
       "         [  7,  11,  12],\n",
       "         [  7,   9,   8],\n",
       "         [  5,   7,   6]],\n",
       "\n",
       "        [[247, 255, 255],\n",
       "         [248, 254, 255],\n",
       "         [250, 255, 255],\n",
       "         ...,\n",
       "         [  5,  12,  10],\n",
       "         [  9,  13,  10],\n",
       "         [  6,  10,   7]],\n",
       "\n",
       "        [[241, 250, 251],\n",
       "         [244, 249, 250],\n",
       "         [245, 251, 250],\n",
       "         ...,\n",
       "         [ 16,  26,  21],\n",
       "         [ 15,  23,  17],\n",
       "         [ 13,  20,  14]]],\n",
       "\n",
       "\n",
       "       [[[ 79,  96, 139],\n",
       "         [ 79,  96, 140],\n",
       "         [ 79,  97, 141],\n",
       "         ...,\n",
       "         [ 77,  94, 138],\n",
       "         [ 76,  93, 138],\n",
       "         [ 76,  93, 137]],\n",
       "\n",
       "        [[ 84, 101, 143],\n",
       "         [ 84, 101, 144],\n",
       "         [ 85, 102, 145],\n",
       "         ...,\n",
       "         [ 82,  99, 142],\n",
       "         [ 81,  98, 141],\n",
       "         [ 81,  98, 140]],\n",
       "\n",
       "        [[ 89, 107, 147],\n",
       "         [ 89, 107, 148],\n",
       "         [ 90, 107, 150],\n",
       "         ...,\n",
       "         [ 89, 105, 146],\n",
       "         [ 89, 104, 146],\n",
       "         [ 88, 104, 145]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[164, 172, 191],\n",
       "         [165, 173, 192],\n",
       "         [165, 173, 192],\n",
       "         ...,\n",
       "         [166, 173, 192],\n",
       "         [165, 172, 191],\n",
       "         [165, 171, 190]],\n",
       "\n",
       "        [[165, 173, 192],\n",
       "         [165, 173, 192],\n",
       "         [166, 174, 193],\n",
       "         ...,\n",
       "         [167, 174, 193],\n",
       "         [166, 173, 192],\n",
       "         [165, 172, 191]],\n",
       "\n",
       "        [[165, 173, 192],\n",
       "         [166, 174, 193],\n",
       "         [167, 175, 194],\n",
       "         ...,\n",
       "         [168, 175, 194],\n",
       "         [167, 174, 193],\n",
       "         [166, 173, 192]]],\n",
       "\n",
       "\n",
       "       [[[ 67, 100,  58],\n",
       "         [ 70, 107,  59],\n",
       "         [ 93, 133,  77],\n",
       "         ...,\n",
       "         [ 75, 100,  70],\n",
       "         [ 85, 111,  73],\n",
       "         [ 86, 112,  74]],\n",
       "\n",
       "        [[ 72, 105,  64],\n",
       "         [ 77, 114,  65],\n",
       "         [ 92, 132,  76],\n",
       "         ...,\n",
       "         [ 71,  96,  65],\n",
       "         [ 92, 116,  90],\n",
       "         [ 91, 116,  89]],\n",
       "\n",
       "        [[ 75, 106,  73],\n",
       "         [ 77, 115,  60],\n",
       "         [ 87, 127,  72],\n",
       "         ...,\n",
       "         [ 66,  85,  62],\n",
       "         [ 84, 108,  86],\n",
       "         [ 87, 112,  90]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 91, 120,  86],\n",
       "         [ 70,  99,  66],\n",
       "         [ 95, 124,  91],\n",
       "         ...,\n",
       "         [141, 162, 138],\n",
       "         [133, 163, 143],\n",
       "         [104, 138, 116]],\n",
       "\n",
       "        [[ 83, 111,  82],\n",
       "         [ 68,  97,  70],\n",
       "         [ 97, 131,  97],\n",
       "         ...,\n",
       "         [132, 155, 128],\n",
       "         [133, 161, 139],\n",
       "         [111, 142, 119]],\n",
       "\n",
       "        [[ 76, 103,  75],\n",
       "         [ 79, 109,  85],\n",
       "         [ 94, 128,  94],\n",
       "         ...,\n",
       "         [127, 151, 124],\n",
       "         [134, 160, 140],\n",
       "         [118, 148, 127]]]], dtype=uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e4daac80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 8, 4, ..., 8, 0, 4])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f863ec2a",
   "metadata": {},
   "source": [
    "# from model zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61cb5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a7bc223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/ILSVRC/imagenet-1k\n",
    "\n",
    "data_dir = '/serenity/scratch/psml/repo/psml/data/ILSVRC2012'\n",
    "\n",
    "imagenet_data = datasets.ImageNet(root=data_dir, split='val', transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(imagenet_data, batch_size=256, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa7ecf8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageNet\n",
       "    Number of datapoints: 50000\n",
       "    Root location: /serenity/scratch/psml/repo/psml/data/ILSVRC2012\n",
       "    Split: val\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
       "               CenterCrop(size=(224, 224))\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagenet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59a153f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "imagenet_inputs = [] \n",
    "imagenet_labels = []\n",
    "\n",
    "iter = 0\n",
    "for images__, labels__ in data_loader:     \n",
    "    print(iter)\n",
    "    iter += 1\n",
    "    imagenet_inputs.append(images__)\n",
    "    imagenet_labels.append(labels__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75fe86c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_inputs = np.concatenate(inputs_, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6d90e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_labels = np.concatenate(labels_, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6126caae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3, 224, 224)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagenet_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f28ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in data_loader:\n",
    "    print(images)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af3e3451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(labels)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ce646a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = functools.partial(convnet.ConvNet, scales=3, filters=32, filters_max=1024,\n",
    "                                 pooling=objax.functional.max_pool_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46bb57f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'functools.partial' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'functools.partial' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34fd0433",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model(3, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03595939",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99f05f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "objax.zoo.convnet.ConvNet(\n",
       "  [0] objax.nn.Conv2D(nin=3, nout=32, k=(3, 3), strides=(1, 1), dilations=(1, 1), groups=1, padding='SAME', use_bias=True, w_init=kaiming_normal(*, gain=1))\n",
       "  [1] leaky_relu(*, negative_slope=0.01)\n",
       "  [2] objax.nn.Conv2D(nin=32, nout=32, k=(3, 3), strides=(1, 1), dilations=(1, 1), groups=1, padding='SAME', use_bias=True, w_init=kaiming_normal(*, gain=1))\n",
       "  [3] leaky_relu(*, negative_slope=0.01)\n",
       "  [4] objax.nn.Conv2D(nin=32, nout=64, k=(3, 3), strides=(1, 1), dilations=(1, 1), groups=1, padding='SAME', use_bias=True, w_init=kaiming_normal(*, gain=1))\n",
       "  [5] leaky_relu(*, negative_slope=0.01)\n",
       "  [6] max_pool_2d(*, size=2, strides=2, padding=ConvPadding.VALID)\n",
       "  [7] objax.nn.Conv2D(nin=64, nout=64, k=(3, 3), strides=(1, 1), dilations=(1, 1), groups=1, padding='SAME', use_bias=True, w_init=kaiming_normal(*, gain=1))\n",
       "  [8] leaky_relu(*, negative_slope=0.01)\n",
       "  [9] objax.nn.Conv2D(nin=64, nout=128, k=(3, 3), strides=(1, 1), dilations=(1, 1), groups=1, padding='SAME', use_bias=True, w_init=kaiming_normal(*, gain=1))\n",
       "  [10] leaky_relu(*, negative_slope=0.01)\n",
       "  [11] max_pool_2d(*, size=2, strides=2, padding=ConvPadding.VALID)\n",
       "  [12] objax.nn.Conv2D(nin=128, nout=128, k=(3, 3), strides=(1, 1), dilations=(1, 1), groups=1, padding='SAME', use_bias=True, w_init=kaiming_normal(*, gain=1))\n",
       "  [13] leaky_relu(*, negative_slope=0.01)\n",
       "  [14] objax.nn.Conv2D(nin=128, nout=256, k=(3, 3), strides=(1, 1), dilations=(1, 1), groups=1, padding='SAME', use_bias=True, w_init=kaiming_normal(*, gain=1))\n",
       "  [15] leaky_relu(*, negative_slope=0.01)\n",
       "  [16] max_pool_2d(*, size=2, strides=2, padding=ConvPadding.VALID)\n",
       "  [17] objax.nn.Conv2D(nin=256, nout=1000, k=(3, 3), strides=(1, 1), dilations=(1, 1), groups=1, padding='SAME', use_bias=True, w_init=kaiming_normal(*, gain=1))\n",
       "  [18] _mean_reduce\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a1c1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-10 22:48:26.829435: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-10 22:48:26.842383: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-10 22:48:26.846202: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-10 22:48:28.257276: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "starting run 9.\n",
      "I1010 22:48:31.987497 140313265325888 dataset_info.py:617] Load dataset info from /nethome/dsanyal7/TFDS/cifar10/3.0.2\n",
      "I1010 22:48:31.991467 140313265325888 dataset_builder.py:579] Reusing dataset cifar10 (/nethome/dsanyal7/TFDS/cifar10/3.0.2)\n",
      "I1010 22:48:31.992088 140313265325888 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /nethome/dsanyal7/TFDS/cifar10/3.0.2.\n",
      "I1010 22:48:32.024960 140313265325888 logging_logger.py:49] Constructing tf.data.Dataset cifar10 for split test, from /nethome/dsanyal7/TFDS/cifar10/3.0.2\n",
      "train:  <function DataSet.__getattr__.<locals>.call_and_update at 0x7f97f062d360>\n",
      "test:  <function DataSet.__getattr__.<locals>.call_and_update at 0x7f97f062d360>\n",
      "xs:  (25073, 32, 32, 3)\n",
      "ys:  (25073,)\n",
      "I1010 22:48:32.329226 140313265325888 xla_bridge.py:895] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "I1010 22:48:32.330105 140313265325888 xla_bridge.py:895] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2024-10-10 22:48:32.389881: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.6.77. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "No checkpoints found. Skipping restoring variables.\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "x:  (256, 3, 32, 32)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "Epoch 0001  Loss 1.78  Accuracy 39.04\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "data['image'].numpy() : (256, 3, 32, 32)\n",
      "data['label'].numpy() : (256, 10)\n",
      "Epoch 0002  Loss 1.41  Accuracy 43.81\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES='0' python3 -u train.py --dataset=cifar10 --epochs=2 --save_steps=2 --arch wrn28-2 --num_experiments 16 --expid 9 --logdir exp/cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09878b09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
