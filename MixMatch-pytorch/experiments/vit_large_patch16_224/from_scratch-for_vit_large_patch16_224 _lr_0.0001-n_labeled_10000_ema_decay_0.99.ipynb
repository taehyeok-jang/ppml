{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee8fbd1-0da2-4f99-8472-ff6a76fcf907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime \n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torch.utils.data as data # for code compatibility\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "import timm\n",
    "\n",
    "import scipy.stats\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# util\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# customized \n",
    "import models.arch as models\n",
    "\n",
    "# import dataset.cifar10 as dataset\n",
    "import dataset.cifar10_larger as dataset # for ViT model,\n",
    "\n",
    "from utils import Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from progress.bar import Bar as Bar\n",
    "import utils as utils_\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "##\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc020f14-3f82-4d5e-91b2-f631b4cb5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e131e4c8-44de-48a2-8262-febbe8fabe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lr=0.02\n",
    "epochs=25\n",
    "n_shadows = 64\n",
    "shadow_id = -1 \n",
    "model = \"efficientnet_b7\"\n",
    "dataset = \"cifar100\"\n",
    "pkeep = 0.5\n",
    "savedir = f\"exp/{model}_{dataset}\"\n",
    "debug = True\n",
    "'''\n",
    "\n",
    "# for vit_large_patch16_224, CIFAR-10\n",
    "_lr = 0.0001\n",
    "_epochs = 50\n",
    "_arch = \"vit_large_patch16_224\"\n",
    "_dataset = \"cifar10\"\n",
    "_n_classes = 10 # depend on dataset\n",
    "_debug = True\n",
    "_batch_size = 32\n",
    "\n",
    "_n_labeled = 10000\n",
    "_n_unlabeled = 45000 - _n_labeled\n",
    "\n",
    "_alpha = 0.75\n",
    "_lambda_u = 20 # default: 75\n",
    "\n",
    "\"\"\"\n",
    "_ema_decay = 0.999 # default: 0.999\n",
    "\"\"\"\n",
    "_ema_decay = 0.99\n",
    "_T = 0.5 # default: 0.5\n",
    "# _train_iteration = int(max(_n_labeled, _n_unlabeled) / _batch_size) # phase 1: support full-supervised learning, default: 1024\n",
    "_train_iteration = int(_n_labeled / _batch_size) # phase 1: support full-supervised learning, default: 1024\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea7019f1-f206-43c4-bb26-556167d635e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiments/vit_large_patch16_224_only/cifar10@10000_lr_0.0001_lu_20_iter_312'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_out = 'experiments/{}_only/{}@{}_lr_{}_lu_{}_iter_{}'.format(_arch, _dataset, _n_labeled, _lr, _lambda_u, _train_iteration)\n",
    "_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e856462c-82ac-4300-a2ed-df226421f646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_train_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfc5d03e-0601-463d-b686-f8e710f9fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1583745484"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e19dd-60f1-4120-b473-65b73342c60d",
   "metadata": {},
   "source": [
    "# prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89657f49-dd3a-4cf7-8a14-d45e5f11a4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd7f65ce350>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622c7797-e64b-4043-933f-2ec877a92fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing cifar10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "#Labeled: 10000 #Unlabeled: 35000 #Val: 5000\n"
     ]
    }
   ],
   "source": [
    "print(f'==> Preparing cifar10')\n",
    "# from cifar10.py\n",
    "cifar10_mean = (0.4914, 0.4822, 0.4465) # equals np.mean(train_set.train_data, axis=(0,1,2))/255\n",
    "cifar10_std = (0.2471, 0.2435, 0.2616) # equals np.std(train_set.train_data, axis=(0,1,2))/255\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "])\n",
    "\n",
    "\n",
    "datadir = Path().home() / \"dataset\"\n",
    "\n",
    "batch_size=_batch_size\n",
    "\n",
    "train_labeled_set, train_unlabeled_set, val_set, test_set = dataset.get_cifar10(datadir, _n_labeled, transform_train=transform_train, transform_val=transform_val)\n",
    "labeled_trainloader = data.DataLoader(train_labeled_set, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "unlabeled_trainloader = data.DataLoader(train_unlabeled_set, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "val_loader = data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\"\"\"\n",
    "test_loader = data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\"\"\"\n",
    "indices = np.random.choice(len(test_set), int(0.2 * len(test_set)), replace=False)\n",
    "subset_test_set = Subset(test_set, indices)\n",
    "test_loader = data.DataLoader(subset_test_set, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05c26ecc-7991-4d55-a4c3-f48fce0828a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10_labeled\n",
       "    Number of datapoints: 10000\n",
       "    Root location: /home/dsanyal7/dataset\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToPILImage()\n",
       "               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616))\n",
       "           )"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labeled_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32d6e59c-5add-4710-b1db-c654c069f9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labeled_set[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea299b71-43e9-4364-9b7e-096b308d39e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10_unlabeled\n",
       "    Number of datapoints: 35000\n",
       "    Root location: /home/dsanyal7/dataset\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: <dataset.cifar10_larger.TransformTwice object at 0x7fd6ea424d90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_unlabeled_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "681b5d93-8201-4c92-b835-2a53bf801d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subset_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aec0abb4-7c83-43b2-8ac2-e8787dfb8012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils_.get_mean_and_std(train_labeled_set)\n",
    "\n",
    "# expected values:\n",
    "# (tensor([ 2.1660e-05, -8.8033e-04,  1.0356e-03]),\n",
    "# tensor([0.8125, 0.8125, 0.7622]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15215480-3dc9-4e0c-a436-b10ed9c92de2",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "374cfee5-12b5-423e-97f8-d4392f80a67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arch: vit_large_patch16_224, pretrained: True, n_classes: 10\n",
      "Freezing ViT-Large intermediate layers...\n",
      "arch: vit_large_patch16_224, pretrained: True, n_classes: 10\n",
      "Freezing ViT-Large intermediate layers...\n"
     ]
    }
   ],
   "source": [
    "def create_model(ema=False):\n",
    "    model = models.network(_arch, pretrained=True, n_classes=_n_classes)\n",
    "    model = model.cuda()\n",
    "\n",
    "    if ema:\n",
    "        for param in model.parameters():\n",
    "            param.detach_()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "ema_model = create_model(ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eadca15-2643-488d-ac28-820a5daaf773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 303.31M\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "print('Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d28b612-7379-4cdd-be6b-7aa9fcf9bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_rampup(current, rampup_length=_epochs):\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        current = np.clip(current / rampup_length, 0.0, 1.0)\n",
    "        return float(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6ef8d9b-d09d-4ace-a16f-a01b2f2ae803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeigthEMA(object):\n",
    "    def __init__(self, model, ema_model, alpha=0.999):\n",
    "        self.model = model\n",
    "        self.ema_model = ema_model\n",
    "        self.alpha = alpha\n",
    "        self.params = list(model.state_dict().values())\n",
    "        self.ema_params = list(ema_model.state_dict().values())\n",
    "        self.wd = 0.02 * _lr # for lr=0.02, wd = 0.0004\n",
    "\n",
    "        for param, ema_param in zip(self.params, self.ema_params):\n",
    "            param.data.copy_(ema_param.data)\n",
    "\n",
    "    def step(self):\n",
    "        one_minus_alpha = 1.0 - self.alpha\n",
    "        for param, ema_param in zip(self.params, self.ema_params):\n",
    "            if param.requires_grad: # don't have to update freezed layers \n",
    "                if ema_param.dtype==torch.float32:\n",
    "                    ema_param.mul_(self.alpha)\n",
    "                    ema_param.add_(param * one_minus_alpha)\n",
    "                    # customized weight decay (TODO: should carefully set up)\n",
    "                    # \n",
    "                    # lr=0.02 -> (0.9996)^5928 (1 epoch): 0.09332.\n",
    "                    # lr=0.002 -> (0.99996)^5928 (1 epoch): 0.78889\n",
    "                    # param.mul_(1 - self.wd) # x 0.9996 (for lr=0.02), x0.99996 (for lr=0\n",
    "\n",
    "class CustomLoss(object):\n",
    "    def __call__(self, outputs_x, targets_x, outputs_u, targets_u, epoch):\n",
    "        probs_u = torch.softmax(outputs_u, dim=1)\n",
    "        \n",
    "        Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n",
    "        \n",
    "        # CHECK:\n",
    "        Lu = torch.mean((probs_u - targets_u)**2)\n",
    "        w = _lambda_u * linear_rampup(epoch) # _lambda_u: 75 (default)\n",
    "        \"\"\" \n",
    "        Lu = 0\n",
    "        w = 0\n",
    "        \"\"\"\n",
    "\n",
    "        return Lx, Lu, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d73f9578-3b1d-4af5-b91d-1166b638e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=_lr, momentum=0.9, weight_decay=5e-4)\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=_epochs)\n",
    "\n",
    "ema_optim = WeigthEMA(model, ema_model, alpha=_ema_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9495f15b-90ca-4652-af79-9dd31e517db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_acc(model, dl):\n",
    "    acc = []\n",
    "    for x, y in dl:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        acc.append(torch.argmax(model(x), dim=1) == y)\n",
    "    acc = torch.cat(acc)\n",
    "    acc = torch.sum(acc) / len(acc)\n",
    "\n",
    "    return acc.item()\n",
    "\n",
    "def interleave_offsets(batch, nu):\n",
    "    groups = [batch // (nu + 1)] * (nu + 1)\n",
    "    for x in range(batch - sum(groups)):\n",
    "        groups[-x - 1] += 1\n",
    "    offsets = [0]\n",
    "    for g in groups:\n",
    "        offsets.append(offsets[-1] + g)\n",
    "    assert offsets[-1] == batch\n",
    "    return offsets\n",
    "\n",
    "def interleave(xy, batch):\n",
    "    nu = len(xy) - 1\n",
    "    offsets = interleave_offsets(batch, nu)\n",
    "    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n",
    "    for i in range(1, nu + 1):\n",
    "        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n",
    "    return [torch.cat(v, dim=0) for v in xy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "babd8351-ccd2-483f-912a-034133111b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, criterion, epoch, use_cuda):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    losses_x = AverageMeter()\n",
    "    losses_u = AverageMeter()\n",
    "    ws = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    bar = Bar('Training', max=_train_iteration)\n",
    "    labeled_train_iter = iter(labeled_trainloader)\n",
    "    unlabeled_train_iter = iter(unlabeled_trainloader)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    with tqdm(range(_train_iteration), desc=\"Training Progress\", unit=\"batch\") as progress_bar:\n",
    "        for batch_idx in progress_bar:\n",
    "            try:\n",
    "                inputs_x, targets_x = next(labeled_train_iter)\n",
    "            except:\n",
    "                labeled_train_iter = iter(labeled_trainloader)\n",
    "                inputs_x, targets_x = next(labeled_train_iter)\n",
    "\n",
    "            try:\n",
    "                (inputs_u, inputs_u2), _ = next(unlabeled_train_iter) # two different augment(x)s;\n",
    "            except:\n",
    "                unlabeled_train_iter = iter(unlabeled_trainloader)\n",
    "                (inputs_u, inputs_u2), _ = next(unlabeled_train_iter)\n",
    "    \n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "            \n",
    "            batch_size = inputs_x.size(0)\n",
    "    \n",
    "            # convert label to one-hot\n",
    "            targets_x = torch.zeros(batch_size, _n_classes).scatter_(1, targets_x.view(-1,1).long(), 1)\n",
    "    \n",
    "            if use_cuda:\n",
    "                inputs_x, targets_x = inputs_x.cuda(), targets_x.cuda(non_blocking=True)\n",
    "                inputs_u = inputs_u.cuda()\n",
    "                inputs_u2 = inputs_u2.cuda()\n",
    "\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # compute guessed labels of unlabel samples\n",
    "                outputs_u = model(inputs_u)\n",
    "                outputs_u2 = model(inputs_u2)\n",
    "                p = (torch.softmax(outputs_u, dim=1) + torch.softmax(outputs_u2, dim=1)) / 2\n",
    "                pt = p**(1/_T)\n",
    "                targets_u = pt / pt.sum(dim=1, keepdim=True)\n",
    "                targets_u = targets_u.detach()\n",
    "            \n",
    "            # mixup \n",
    "            all_inputs = torch.cat([inputs_x, inputs_u, inputs_u2], dim=0)\n",
    "            all_targets = torch.cat([targets_x, targets_u, targets_u], dim=0)\n",
    "            \"\"\"\n",
    "            all_inputs = torch.cat([inputs_x], dim=0)\n",
    "            all_targets = torch.cat([targets_x], dim=0)\n",
    "            \"\"\"\n",
    "            \n",
    "            # CHECK: \n",
    "            l = np.random.beta(_alpha, _alpha)\n",
    "            \"\"\"\n",
    "            l = 1\n",
    "            \"\"\"\n",
    "            \n",
    "            l = max(l, 1-l)\n",
    "            \n",
    "            idx = torch.randperm(all_inputs.size(0))\n",
    "            \n",
    "            input_a, input_b = all_inputs, all_inputs[idx]\n",
    "            target_a, target_b = all_targets, all_targets[idx]\n",
    "            \n",
    "            # input_a > input_b is guaranteed.\n",
    "            mixed_input = l * input_a + (1 - l) * input_b\n",
    "            mixed_target = l * target_a + (1 - l) * target_b\n",
    "\n",
    "            # interleave labeled and unlabed samples between batches to get correct batchnorm calculation \n",
    "            mixed_input = list(torch.split(mixed_input, batch_size))\n",
    "            mixed_input = interleave(mixed_input, batch_size)\n",
    "            \n",
    "            logits = [model(mixed_input[0])]\n",
    "            for input in mixed_input[1:]:\n",
    "                logits.append(model(input))\n",
    "            \"\"\"\n",
    "            logits = [model(mixed_input)]\n",
    "            \"\"\"\n",
    "            \n",
    "            # put interleaved samples back\n",
    "            logits = interleave(logits, batch_size)\n",
    "            logits_x = logits[0]\n",
    "            logits_u = torch.cat(logits[1:], dim=0)\n",
    "            \"\"\"\n",
    "            logits_x = logits[0]\n",
    "            \"\"\"\n",
    "\n",
    "            Lx, Lu, w = criterion(logits_x, mixed_target[:batch_size], logits_u, mixed_target[batch_size:], epoch+batch_idx/_train_iteration)\n",
    "            \"\"\"\n",
    "            Lx, Lu, w = criterion(logits_x, mixed_target[:batch_size], None, None, epoch+batch_idx/_train_iteration)\n",
    "            \"\"\"\n",
    "    \n",
    "            loss = Lx + w * Lu\n",
    "    \n",
    "            losses.update(loss, inputs_x.size(0))\n",
    "            losses_x.update(Lx, inputs_x.size(0))\n",
    "            losses_u.update(Lu, inputs_x.size(0))\n",
    "            ws.update(w, inputs_x.size(0))\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            # CHECK:\n",
    "            ema_optimizer.step()\n",
    "    \n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "        \n",
    "            # plot progress\n",
    "            bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | Loss_x: {loss_x:.4f} | Loss_u: {loss_u:.4f} | W: {w:.4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=_train_iteration,\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        total=bar.elapsed_td,\n",
    "                        eta=bar.eta_td,\n",
    "                        loss=losses.avg,\n",
    "                        loss_x=losses_x.avg,\n",
    "                        loss_u=losses_u.avg,\n",
    "                        w=ws.avg,\n",
    "                        )\n",
    "            bar.next()\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                \"Data\": f\"{data_time.avg:.3f}s\",\n",
    "                \"Batch\": f\"{batch_time.avg:.3f}s\",\n",
    "                \"Total\": bar.elapsed_td,\n",
    "                \"ETA\": bar.eta_td,\n",
    "                \"Loss\": f\"{losses.avg:.4f}\",\n",
    "                \"Loss_x\": f\"{losses_x.avg:.4f}\",\n",
    "                \"Loss_u\": f\"{losses_u.avg:.4f}\",\n",
    "                \"W\": f\"{ws.avg:.4f}\",\n",
    "            })\n",
    "            \n",
    "        bar.finish()\n",
    "    \n",
    "    return (losses.avg, losses_x.avg, losses_u.avg,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69624bbd-d585-4cf5-9cd2-c8981f92abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valloader, model, criterion, epoch, use_cuda, mode):\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar(f'{mode}', max=len(valloader))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # plot progress\n",
    "            bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(valloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        total=bar.elapsed_td,\n",
    "                        eta=bar.eta_td,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        )\n",
    "            bar.next()\n",
    "        bar.finish()\n",
    "        \n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e10b10cf-e28d-494c-9124-25ce540d1cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_criterion = CustomLoss()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3365861a-a9de-4b6c-81c8-533cec870129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 312/312 [06:53<00:00,  1.33s/batch, Data=0.762s, Batch=1.324s, Total=0:06:53, ETA=0:00:00, Loss=1.5456, Loss_x=1.5421, Loss_u=0.0173, W=0.1994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] (naive) Test Accuracy: 0.9530\n",
      "[Epoch 0] Train Accuracy: 0.0000\n",
      "[Epoch 0] Validation Accuracy: 0.0000\n",
      "[Epoch 0] Test Accuracy: 95.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 312/312 [06:55<00:00,  1.33s/batch, Data=0.770s, Batch=1.329s, Total=0:06:55, ETA=0:00:00, Loss=1.3256, Loss_x=1.3156, Loss_u=0.0169, W=0.5994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] (naive) Test Accuracy: 0.9670\n",
      "[Epoch 1] Train Accuracy: 0.0000\n",
      "[Epoch 1] Validation Accuracy: 0.0000\n",
      "[Epoch 1] Test Accuracy: 96.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 312/312 [06:54<00:00,  1.33s/batch, Data=0.770s, Batch=1.328s, Total=0:06:55, ETA=0:00:00, Loss=1.3347, Loss_x=1.3177, Loss_u=0.0170, W=0.9994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] (naive) Test Accuracy: 0.9705\n",
      "[Epoch 2] Train Accuracy: 0.0000\n",
      "[Epoch 2] Validation Accuracy: 0.0000\n",
      "[Epoch 2] Test Accuracy: 97.0500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 312/312 [06:55<00:00,  1.33s/batch, Data=0.770s, Batch=1.329s, Total=0:06:55, ETA=0:00:00, Loss=1.2997, Loss_x=1.2772, Loss_u=0.0161, W=1.3994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] (naive) Test Accuracy: 0.9700\n",
      "[Epoch 3] Train Accuracy: 0.0000\n",
      "[Epoch 3] Validation Accuracy: 0.0000\n",
      "[Epoch 3] Test Accuracy: 97.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 312/312 [06:55<00:00,  1.33s/batch, Data=0.770s, Batch=1.330s, Total=0:06:55, ETA=0:00:00, Loss=1.2416, Loss_x=1.2140, Loss_u=0.0153, W=1.7994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] (naive) Test Accuracy: 0.9740\n",
      "[Epoch 4] Train Accuracy: 0.0000\n",
      "[Epoch 4] Validation Accuracy: 0.0000\n",
      "[Epoch 4] Test Accuracy: 97.4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 312/312 [06:54<00:00,  1.33s/batch, Data=0.769s, Batch=1.326s, Total=0:06:54, ETA=0:00:00, Loss=1.2726, Loss_x=1.2368, Loss_u=0.0162, W=2.1994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] (naive) Test Accuracy: 0.9730\n",
      "[Epoch 5] Train Accuracy: 0.0000\n",
      "[Epoch 5] Validation Accuracy: 0.0000\n",
      "[Epoch 5] Test Accuracy: 97.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  39%|███▉      | 121/312 [02:42<04:16,  1.34s/batch, Data=0.769s, Batch=1.329s, Total=0:02:41, ETA=0:04:15, Loss=1.2449, Loss_x=1.2072, Loss_u=0.0152, W=2.4769]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m test_accs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(_epochs):\n\u001b[0;32m---> 16\u001b[0m     train_loss, train_loss_x, train_loss_u \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabeled_trainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlabeled_trainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mema_optim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    _, train_acc = validate(labeled_trainloader, ema_model, criterion, epoch, use_cuda, mode='Train Stats')\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    val_loss, val_acc = validate(val_loader, ema_model, criterion, epoch, use_cuda, mode='Valid Stats')\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    test_loss, test_acc = validate(test_loader, ema_model, criterion, epoch, use_cuda, mode='Test Stats ')\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     _, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m\n",
      "Cell \u001b[0;32mIn[21], line 123\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, criterion, epoch, use_cuda)\u001b[0m\n\u001b[1;32m    120\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# plot progress\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m bar\u001b[38;5;241m.\u001b[39msuffix  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;132;43;01m{batch}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{size}\u001b[39;49;00m\u001b[38;5;124;43m) Data: \u001b[39;49m\u001b[38;5;132;43;01m{data:.3f}\u001b[39;49;00m\u001b[38;5;124;43ms | Batch: \u001b[39;49m\u001b[38;5;132;43;01m{bt:.3f}\u001b[39;49;00m\u001b[38;5;124;43ms | Total: \u001b[39;49m\u001b[38;5;132;43;01m{total:}\u001b[39;49;00m\u001b[38;5;124;43m | ETA: \u001b[39;49m\u001b[38;5;132;43;01m{eta:}\u001b[39;49;00m\u001b[38;5;124;43m | Loss: \u001b[39;49m\u001b[38;5;132;43;01m{loss:.4f}\u001b[39;49;00m\u001b[38;5;124;43m | Loss_x: \u001b[39;49m\u001b[38;5;132;43;01m{loss_x:.4f}\u001b[39;49;00m\u001b[38;5;124;43m | Loss_u: \u001b[39;49m\u001b[38;5;132;43;01m{loss_u:.4f}\u001b[39;49;00m\u001b[38;5;124;43m | W: \u001b[39;49m\u001b[38;5;132;43;01m{w:.4f}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m            \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_train_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_time\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_time\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melapsed_td\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m            \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meta_td\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloss_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlosses_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloss_u\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlosses_u\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mws\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m bar\u001b[38;5;241m.\u001b[39mnext()\n\u001b[1;32m    137\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mset_postfix({\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_time\u001b[38;5;241m.\u001b[39mavg\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_time\u001b[38;5;241m.\u001b[39mavg\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mws\u001b[38;5;241m.\u001b[39mavg\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    146\u001b[0m })\n",
      "File \u001b[0;32m~/miniconda3/envs/mia/lib/python3.8/site-packages/torch/_tensor.py:986\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m--> 986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(_out):\n",
    "    mkdir_p(_out)\n",
    "\n",
    "title = 'noisy-cifar-10'\n",
    "\n",
    "logger = Logger(os.path.join(_out, 'log.txt'), title=title)\n",
    "logger.set_names(['Train Loss', 'Train Loss X', 'Train Loss U',  'Valid Loss', 'Valid Acc.', 'Test Loss', 'Test Acc.'])\n",
    "writer = SummaryWriter(_out)\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "step = 0\n",
    "test_accs = []\n",
    "\n",
    "for epoch in range(_epochs):\n",
    "\n",
    "    train_loss, train_loss_x, train_loss_u = train(labeled_trainloader, unlabeled_trainloader, model, optim, ema_optim, train_criterion, epoch, use_cuda)\n",
    "    \"\"\"\n",
    "    _, train_acc = validate(labeled_trainloader, ema_model, criterion, epoch, use_cuda, mode='Train Stats')\n",
    "    val_loss, val_acc = validate(val_loader, ema_model, criterion, epoch, use_cuda, mode='Valid Stats')\n",
    "    test_loss, test_acc = validate(test_loader, ema_model, criterion, epoch, use_cuda, mode='Test Stats ')\n",
    "    \"\"\"\n",
    "    _, train_acc = 0.0, 0.0\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    test_loss, test_acc = validate(test_loader, model, criterion, epoch, use_cuda, mode='Test Stats ')\n",
    "\n",
    "    sched.step()\n",
    "\n",
    "    _test_acc = get_acc(model, test_loader)\n",
    "\n",
    "    print(f\"[Epoch {epoch}] (naive) Test Accuracy: {_test_acc:.4f}\")\n",
    "    print(f\"[Epoch {epoch}] Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"[Epoch {epoch}] Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"[Epoch {epoch}] Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    \n",
    "    step = _train_iteration * (epoch + 1)\n",
    "\n",
    "    writer.add_scalar('losses/train_loss', train_loss, step)\n",
    "    writer.add_scalar('losses/valid_loss', val_loss, step)\n",
    "    writer.add_scalar('losses/test_loss', test_loss, step)\n",
    "\n",
    "    writer.add_scalar('accuracy/train_acc', train_acc, step)\n",
    "    writer.add_scalar('accuracy/val_acc', val_acc, step)\n",
    "    writer.add_scalar('accuracy/test_acc', test_acc, step)\n",
    "\n",
    "    logger.append([train_loss, train_loss_x, train_loss_u, val_loss, val_acc, test_loss, test_acc])\n",
    "\n",
    "    best_acc = max(val_acc, best_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "logger.close()\n",
    "writer.close()\n",
    "\n",
    "print('Best acc:')\n",
    "print(best_acc)\n",
    "\n",
    "print('Mean acc:')\n",
    "print(np.mean(test_accs[-20:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb67fc-c769-44de-ae04-3e94b66ffbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35896151-1191-4ebc-8ea2-fcfb89e2a0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
