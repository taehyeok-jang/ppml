{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee8fbd1-0da2-4f99-8472-ff6a76fcf907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime \n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torch.utils.data as data # for code compatibility\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "import timm\n",
    "\n",
    "import scipy.stats\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# util\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# customized \n",
    "import models.arch as models\n",
    "\n",
    "import dataset.cifar10_larger as cifar10_  # for ViT model,\n",
    "import dataset.cifar100_larger as cifar100_   # for ViT model,\n",
    "\n",
    "from utils import Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from progress.bar import Bar as Bar\n",
    "import utils as utils_\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "##\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc020f14-3f82-4d5e-91b2-f631b4cb5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e131e4c8-44de-48a2-8262-febbe8fabe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vit_large_patch16_224, CIFAR-10\n",
    "_lr = 0.005\n",
    "_epochs = 50\n",
    "_arch = \"efficientnet_b7\"\n",
    "_dataset = \"large_cifar100\"\n",
    "_n_classes = 100 # depend on dataset\n",
    "_debug = True\n",
    "_batch_size = 32\n",
    "\n",
    "_n_labeled = 40000\n",
    "_n_unlabeled = 45000 - _n_labeled\n",
    "\n",
    "_alpha = 0.75\n",
    "_lambda_u = 20 # default: 75\n",
    "\n",
    "_ema_decay = 0.999 # default: 0.999\n",
    "_T = 0.5 # default: 0.5\n",
    "# _train_iteration = int(max(_n_labeled, _n_unlabeled) / _batch_size) # phase 1: support full-supervised learning, default: 1024\n",
    "_train_iteration = int(_n_labeled / _batch_size) # phase 1: support full-supervised learning, default: 1024\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea7019f1-f206-43c4-bb26-556167d635e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiments/efficientnet_b7_only/large_cifar100@40000_lr_0.005'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_out = 'experiments/{}_only/{}@{}_lr_{}'.format(_arch, _dataset, _n_labeled, _lr)\n",
    "_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e856462c-82ac-4300-a2ed-df226421f646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_train_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfc5d03e-0601-463d-b686-f8e710f9fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1583745484"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e19dd-60f1-4120-b473-65b73342c60d",
   "metadata": {},
   "source": [
    "# prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89657f49-dd3a-4cf7-8a14-d45e5f11a4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5ef0f85310>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "622c7797-e64b-4043-933f-2ec877a92fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing large_cifar100\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "#Labeled: 40000 #Unlabeled: 5000 #Val: 5000\n"
     ]
    }
   ],
   "source": [
    "print(f'==> Preparing {_dataset}')\n",
    "\n",
    "cifar10_mean = (0.4914, 0.4822, 0.4465) \n",
    "cifar10_std = (0.2471, 0.2435, 0.2616) \n",
    "cifar100_mean = (0.5071, 0.4867, 0.4408)\n",
    "cifar100_std = (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "if _dataset == \"cifar10\" or _dataset == \"large_cifar10\":\n",
    "    mean, std = cifar10_mean, cifar10_std\n",
    "elif _dataset == \"cifar100\" or _dataset == \"large_cifar100\":\n",
    "    mean, std = cifar100_mean, cifar100_std\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported dataset: {_dataset}\")\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "datadir = Path().home() / \"dataset\"\n",
    "\n",
    "batch_size=_batch_size\n",
    "\n",
    "# measure time for data pre-processing\n",
    "start_time = time.time()\n",
    "\n",
    "if _dataset == \"cifar10\" or _dataset == \"large_cifar10\":\n",
    "    train_labeled_set, train_unlabeled_set, val_set, test_set = cifar10_.get_cifar10(datadir, _.n_labeled, transform_train=transform_train, transform_val=transform_val)\n",
    "elif _dataset == \"cifar100\" or _dataset == \"large_cifar100\":\n",
    "    train_labeled_set, train_unlabeled_set, val_set, test_set = cifar100_.get_cifar100(datadir, _n_labeled, transform_train=transform_train, transform_val=transform_val)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported dataset: {_dataset}\")\n",
    "\n",
    "labeled_trainloader = data.DataLoader(train_labeled_set, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "unlabeled_trainloader = data.DataLoader(train_unlabeled_set, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "val_loader = data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\"\"\"\n",
    "test_loader = data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\"\"\"\n",
    "indices = np.random.choice(len(test_set), int(0.2 * len(test_set)), replace=False)\n",
    "subset_test_set = Subset(test_set, indices)\n",
    "test_loader = data.DataLoader(subset_test_set, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05c26ecc-7991-4d55-a4c3-f48fce0828a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR100_labeled\n",
       "    Number of datapoints: 40000\n",
       "    Root location: /home/dsanyal7/dataset\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToPILImage()\n",
       "               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761))\n",
       "           )"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labeled_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32d6e59c-5add-4710-b1db-c654c069f9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labeled_set[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea299b71-43e9-4364-9b7e-096b308d39e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR100_unlabeled\n",
       "    Number of datapoints: 5000\n",
       "    Root location: /home/dsanyal7/dataset\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: <dataset.cifar100_larger.TransformTwice object at 0x7f5de2c69190>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_unlabeled_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd5fe6e0-c6f3-44cf-b354-baa0b0ed0ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subset_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aec0abb4-7c83-43b2-8ac2-e8787dfb8012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils_.get_mean_and_std(train_labeled_set)\n",
    "\n",
    "# expected values:\n",
    "# (tensor([ 2.1660e-05, -8.8033e-04,  1.0356e-03]),\n",
    "# tensor([0.8125, 0.8125, 0.7622]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15215480-3dc9-4e0c-a436-b10ed9c92de2",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "374cfee5-12b5-423e-97f8-d4392f80a67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arch: efficientnet_b7, pretrained: True, n_classes: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsanyal7/miniconda3/envs/mia/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dsanyal7/miniconda3/envs/mia/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B7_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B7_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not freeze layers for model: efficientnet_b7\n",
      "arch: efficientnet_b7, pretrained: True, n_classes: 100\n",
      "Do not freeze layers for model: efficientnet_b7\n"
     ]
    }
   ],
   "source": [
    "def create_model(ema=False):\n",
    "    model = models.network(_arch, pretrained=True, n_classes=_n_classes)\n",
    "    model = model.cuda()\n",
    "\n",
    "    if ema:\n",
    "        for param in model.parameters():\n",
    "            param.detach_()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "ema_model = create_model(ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eadca15-2643-488d-ac28-820a5daaf773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 64.04M\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "print('Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d28b612-7379-4cdd-be6b-7aa9fcf9bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_rampup(current, rampup_length=_epochs):\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        current = np.clip(current / rampup_length, 0.0, 1.0)\n",
    "        return float(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6ef8d9b-d09d-4ace-a16f-a01b2f2ae803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeigthEMA(object):\n",
    "    def __init__(self, model, ema_model, alpha=0.999):\n",
    "        self.model = model\n",
    "        self.ema_model = ema_model\n",
    "        self.alpha = alpha\n",
    "        self.params = list(model.state_dict().values())\n",
    "        self.ema_params = list(ema_model.state_dict().values())\n",
    "        self.wd = 0.02 * _lr # for lr=0.02, wd = 0.0004\n",
    "\n",
    "        for param, ema_param in zip(self.params, self.ema_params):\n",
    "            param.data.copy_(ema_param.data)\n",
    "\n",
    "    def step(self):\n",
    "        one_minus_alpha = 1.0 - self.alpha\n",
    "        for param, ema_param in zip(self.params, self.ema_params):\n",
    "            if ema_param.dtype==torch.float32:\n",
    "                ema_param.mul_(self.alpha)\n",
    "                ema_param.add_(param * one_minus_alpha)\n",
    "                # customized weight decay (TODO: should carefully set up)\n",
    "                # \n",
    "                # lr=0.02 -> (0.9996)^5928 (1 epoch): 0.09332.\n",
    "                # lr=0.002 -> (0.99996)^5928 (1 epoch): 0.78889\n",
    "                # param.mul_(1 - self.wd) # x 0.9996 (for lr=0.02), x0.99996 (for lr=0\n",
    "\n",
    "class CustomLoss(object):\n",
    "    def __call__(self, outputs_x, targets_x, outputs_u, targets_u, epoch):\n",
    "        \"\"\"\n",
    "        probs_u = torch.softmax(outputs_u, dim=1)\n",
    "        \"\"\"\n",
    "        \n",
    "        Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n",
    "        \n",
    "        # CHECK\n",
    "        \"\"\"\n",
    "        Lu = torch.mean((probs_u - targets_u)**2)\n",
    "        w = _lambda_u * linear_rampup(epoch) # _lambda_u: 75 (default)\n",
    "        \"\"\" \n",
    "        Lu = 0\n",
    "        w = 0\n",
    "\n",
    "        return Lx, Lu, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d73f9578-3b1d-4af5-b91d-1166b638e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=_lr, momentum=0.9, weight_decay=5e-4)\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=_epochs)\n",
    "\n",
    "ema_optim = WeigthEMA(model, ema_model, alpha=_ema_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9495f15b-90ca-4652-af79-9dd31e517db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_acc(model, dl):\n",
    "    acc = []\n",
    "    for x, y in dl:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        acc.append(torch.argmax(model(x), dim=1) == y)\n",
    "    acc = torch.cat(acc)\n",
    "    acc = torch.sum(acc) / len(acc)\n",
    "\n",
    "    return acc.item()\n",
    "\n",
    "def interleave_offsets(batch, nu):\n",
    "    groups = [batch // (nu + 1)] * (nu + 1)\n",
    "    for x in range(batch - sum(groups)):\n",
    "        groups[-x - 1] += 1\n",
    "    offsets = [0]\n",
    "    for g in groups:\n",
    "        offsets.append(offsets[-1] + g)\n",
    "    assert offsets[-1] == batch\n",
    "    return offsets\n",
    "\n",
    "def interleave(xy, batch):\n",
    "    nu = len(xy) - 1\n",
    "    offsets = interleave_offsets(batch, nu)\n",
    "    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n",
    "    for i in range(1, nu + 1):\n",
    "        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n",
    "    return [torch.cat(v, dim=0) for v in xy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "babd8351-ccd2-483f-912a-034133111b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, criterion, epoch, use_cuda):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    losses_x = AverageMeter()\n",
    "    losses_u = AverageMeter()\n",
    "    ws = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    bar = Bar('Training', max=_train_iteration)\n",
    "    labeled_train_iter = iter(labeled_trainloader)\n",
    "    unlabeled_train_iter = iter(unlabeled_trainloader)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    with tqdm(range(_train_iteration), desc=\"Training Progress\", unit=\"batch\") as progress_bar:\n",
    "        for batch_idx in progress_bar:\n",
    "            try:\n",
    "                inputs_x, targets_x = next(labeled_train_iter)\n",
    "            except:\n",
    "                labeled_train_iter = iter(labeled_trainloader)\n",
    "                inputs_x, targets_x = next(labeled_train_iter)\n",
    "\n",
    "            \"\"\"\n",
    "            try:\n",
    "                (inputs_u, inputs_u2), _ = next(unlabeled_train_iter) # two different augment(x)s;\n",
    "            except:\n",
    "                unlabeled_train_iter = iter(unlabeled_trainloader)\n",
    "                (inputs_u, inputs_u2), _ = next(unlabeled_train_iter)\n",
    "                \"\"\"\n",
    "    \n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "            \n",
    "            batch_size = inputs_x.size(0)\n",
    "    \n",
    "            # convert label to one-hot\n",
    "            targets_x = torch.zeros(batch_size, _n_classes).scatter_(1, targets_x.view(-1,1).long(), 1)\n",
    "    \n",
    "            if use_cuda:\n",
    "                inputs_x, targets_x = inputs_x.cuda(), targets_x.cuda(non_blocking=True)\n",
    "                \"\"\"\n",
    "                inputs_u = inputs_u.cuda()\n",
    "                inputs_u2 = inputs_u2.cuda()\n",
    "                \"\"\"\n",
    "\n",
    "            \"\"\"\n",
    "            with torch.no_grad():\n",
    "                # compute guessed labels of unlabel samples\n",
    "                outputs_u = model(inputs_u)\n",
    "                outputs_u2 = model(inputs_u2)\n",
    "                p = (torch.softmax(outputs_u, dim=1) + torch.softmax(outputs_u2, dim=1)) / 2\n",
    "                pt = p**(1/_T)\n",
    "                targets_u = pt / pt.sum(dim=1, keepdim=True)\n",
    "                targets_u = targets_u.detach()\n",
    "            \"\"\"\n",
    "            \n",
    "            # mixup \n",
    "            \"\"\" \n",
    "            all_inputs = torch.cat([inputs_x, inputs_u, inputs_u2], dim=0)\n",
    "            all_targets = torch.cat([targets_x, targets_u, targets_u], dim=0)\n",
    "            \"\"\"\n",
    "            all_inputs = torch.cat([inputs_x], dim=0)\n",
    "            all_targets = torch.cat([targets_x], dim=0)\n",
    "\n",
    "            # CHECK: \n",
    "            \"\"\"\n",
    "            l = np.random.beta(_alpha, _alpha)\n",
    "            \"\"\"\n",
    "            l = 1\n",
    "            l = max(l, 1-l)\n",
    "            \n",
    "            idx = torch.randperm(all_inputs.size(0))\n",
    "            \n",
    "            input_a, input_b = all_inputs, all_inputs[idx]\n",
    "            target_a, target_b = all_targets, all_targets[idx]\n",
    "            \n",
    "            # input_a > input_b is guaranteed.\n",
    "            mixed_input = l * input_a + (1 - l) * input_b\n",
    "            mixed_target = l * target_a + (1 - l) * target_b\n",
    "\n",
    "            \"\"\"\n",
    "            # interleave labeled and unlabed samples between batches to get correct batchnorm calculation \n",
    "            mixed_input = list(torch.split(mixed_input, batch_size))\n",
    "            mixed_input = interleave(mixed_input, batch_size)\n",
    "            \"\"\"\n",
    "\n",
    "            \"\"\"\n",
    "            logits = [model(mixed_input[0])]\n",
    "            for input in mixed_input[1:]:\n",
    "                logits.append(model(input))\n",
    "            \"\"\"\n",
    "            logits = [model(mixed_input)]\n",
    "    \n",
    "            # put interleaved samples back\n",
    "            \"\"\"\n",
    "            logits = interleave(logits, batch_size)\n",
    "            logits_x = logits[0]\n",
    "            logits_u = torch.cat(logits[1:], dim=0)\n",
    "            \"\"\"\n",
    "            logits_x = logits[0]\n",
    "\n",
    "            \"\"\"\n",
    "            Lx, Lu, w = criterion(logits_x, mixed_target[:batch_size], logits_u, mixed_target[batch_size:], epoch+batch_idx/_train_iteration)\n",
    "            \"\"\"\n",
    "            Lx, Lu, w = criterion(logits_x, mixed_target[:batch_size], None, None, epoch+batch_idx/_train_iteration)\n",
    "    \n",
    "            loss = Lx + w * Lu\n",
    "    \n",
    "            losses.update(loss, inputs_x.size(0))\n",
    "            losses_x.update(Lx, inputs_x.size(0))\n",
    "            losses_u.update(Lu, inputs_x.size(0))\n",
    "            ws.update(w, inputs_x.size(0))\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            # CHECK \n",
    "            # ema_optimizer.step()\n",
    "    \n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "        \n",
    "            # plot progress\n",
    "            bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | Loss_x: {loss_x:.4f} | Loss_u: {loss_u:.4f} | W: {w:.4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=_train_iteration,\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        total=bar.elapsed_td,\n",
    "                        eta=bar.eta_td,\n",
    "                        loss=losses.avg,\n",
    "                        loss_x=losses_x.avg,\n",
    "                        loss_u=losses_u.avg,\n",
    "                        w=ws.avg,\n",
    "                        )\n",
    "            bar.next()\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                \"Data\": f\"{data_time.avg:.3f}s\",\n",
    "                \"Batch\": f\"{batch_time.avg:.3f}s\",\n",
    "                \"Total\": bar.elapsed_td,\n",
    "                \"ETA\": bar.eta_td,\n",
    "                \"Loss\": f\"{losses.avg:.4f}\",\n",
    "                \"Loss_x\": f\"{losses_x.avg:.4f}\",\n",
    "                \"Loss_u\": f\"{losses_u.avg:.4f}\",\n",
    "                \"W\": f\"{ws.avg:.4f}\",\n",
    "            })\n",
    "            \n",
    "        bar.finish()\n",
    "    \n",
    "    return (losses.avg, losses_x.avg, losses_u.avg,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69624bbd-d585-4cf5-9cd2-c8981f92abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valloader, model, criterion, epoch, use_cuda, mode):\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar(f'{mode}', max=len(valloader))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # plot progress\n",
    "            bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(valloader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        total=bar.elapsed_td,\n",
    "                        eta=bar.eta_td,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        top5=top5.avg,\n",
    "                        )\n",
    "            bar.next()\n",
    "        bar.finish()\n",
    "        \n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f1bbc1a-0720-47c6-8688-3c3b5c660482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint=_out, filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e10b10cf-e28d-494c-9124-25ce540d1cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_criterion = CustomLoss()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3365861a-a9de-4b6c-81c8-533cec870129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1250/1250 [07:22<00:00,  2.83batch/s, Data=0.164s, Batch=0.354s, Total=0:07:22, ETA=0:00:00, Loss=2.7149, Loss_x=2.7149, Loss_u=0.0000, W=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] (naive) Test Accuracy: 0.7660\n",
      "[Epoch 0] Validation Accuracy: 77.7400\n",
      "[Epoch 0] Test Accuracy: 76.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1250/1250 [07:04<00:00,  2.95batch/s, Data=0.158s, Batch=0.340s, Total=0:07:04, ETA=0:00:00, Loss=1.3993, Loss_x=1.3993, Loss_u=0.0000, W=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] (naive) Test Accuracy: 0.8060\n",
      "[Epoch 1] Validation Accuracy: 81.6400\n",
      "[Epoch 1] Test Accuracy: 80.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1250/1250 [07:05<00:00,  2.94batch/s, Data=0.159s, Batch=0.340s, Total=0:07:05, ETA=0:00:00, Loss=1.1734, Loss_x=1.1734, Loss_u=0.0000, W=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] (naive) Test Accuracy: 0.8425\n",
      "[Epoch 2] Validation Accuracy: 83.6200\n",
      "[Epoch 2] Test Accuracy: 84.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1250/1250 [07:03<00:00,  2.95batch/s, Data=0.158s, Batch=0.339s, Total=0:07:04, ETA=0:00:00, Loss=1.0485, Loss_x=1.0485, Loss_u=0.0000, W=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] (naive) Test Accuracy: 0.8455\n",
      "[Epoch 3] Validation Accuracy: 84.6000\n",
      "[Epoch 3] Test Accuracy: 84.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1250/1250 [07:04<00:00,  2.95batch/s, Data=0.157s, Batch=0.340s, Total=0:07:04, ETA=0:00:00, Loss=0.9662, Loss_x=0.9662, Loss_u=0.0000, W=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] (naive) Test Accuracy: 0.8550\n",
      "[Epoch 4] Validation Accuracy: 85.0400\n",
      "[Epoch 4] Test Accuracy: 85.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1250/1250 [07:03<00:00,  2.95batch/s, Data=0.159s, Batch=0.339s, Total=0:07:04, ETA=0:00:00, Loss=0.8923, Loss_x=0.8923, Loss_u=0.0000, W=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] (naive) Test Accuracy: 0.8645\n",
      "[Epoch 5] Validation Accuracy: 84.4000\n",
      "[Epoch 5] Test Accuracy: 86.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  84%|████████▎ | 1046/1250 [05:54<01:09,  2.95batch/s, Data=0.160s, Batch=0.339s, Total=0:05:55, ETA=0:01:09, Loss=0.8503, Loss_x=0.8503, Loss_u=0.0000, W=0.0000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m test_accs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(_epochs):\n\u001b[0;32m---> 16\u001b[0m     train_loss, train_loss_x, train_loss_u \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabeled_trainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munlabeled_trainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mema_optim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    _, train_acc = validate(labeled_trainloader, ema_model, criterion, epoch, use_cuda, mode='Train Stats')\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    val_loss, val_acc = validate(val_loader, ema_model, criterion, epoch, use_cuda, mode='Valid Stats')\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    test_loss, test_acc = validate(test_loader, ema_model, criterion, epoch, use_cuda, mode='Test Stats ')\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     _, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m\n",
      "Cell \u001b[0;32mIn[20], line 118\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, criterion, epoch, use_cuda)\u001b[0m\n\u001b[1;32m    115\u001b[0m ws\u001b[38;5;241m.\u001b[39mupdate(w, inputs_x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    117\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 118\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# CHECK \u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# ema_optimizer.step()\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# measure elapsed time\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mia/lib/python3.8/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mia/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mia/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(_out):\n",
    "    mkdir_p(_out)\n",
    "\n",
    "title = f'noisy-{_dataset}'\n",
    "\n",
    "logger = Logger(os.path.join(_out, 'log.txt'), title=title)\n",
    "logger.set_names(['Train Loss', 'Train Loss X', 'Train Loss U',  'Valid Loss', 'Valid Acc.', 'Test Loss', 'Test Acc.'])\n",
    "writer = SummaryWriter(_out)\n",
    "\n",
    "best_acc = 0  # best test accuracy\n",
    "step = 0\n",
    "test_accs = []\n",
    "\n",
    "for epoch in range(_epochs):\n",
    "\n",
    "    train_loss, train_loss_x, train_loss_u = train(labeled_trainloader, unlabeled_trainloader, model, optim, ema_optim, train_criterion, epoch, use_cuda)\n",
    "    \"\"\"\n",
    "    _, train_acc = validate(labeled_trainloader, ema_model, criterion, epoch, use_cuda, mode='Train Stats')\n",
    "    val_loss, val_acc = validate(val_loader, ema_model, criterion, epoch, use_cuda, mode='Valid Stats')\n",
    "    test_loss, test_acc = validate(test_loader, ema_model, criterion, epoch, use_cuda, mode='Test Stats ')\n",
    "    \"\"\"\n",
    "    _, train_acc = 0.0, 0.0\n",
    "    val_loss, val_acc = validate(val_loader, model, criterion, epoch, use_cuda, mode='Valid Stats')\n",
    "    test_loss, test_acc = validate(test_loader, model, criterion, epoch, use_cuda, mode='Test Stats ')\n",
    "\n",
    "    sched.step()\n",
    "\n",
    "    _test_acc = get_acc(model, test_loader)\n",
    "    \"\"\"\n",
    "    print(f\"[Epoch {epoch}] (naive) Test Accuracy: {_test_acc:.4f}\")\n",
    "    print(f\"[Epoch {epoch}] Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"[Epoch {epoch}] Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"[Epoch {epoch}] Test Accuracy: {test_acc:.4f}\")\n",
    "    \"\"\"\n",
    "    print(f\"[Epoch {epoch}] (naive) Test Accuracy: {_test_acc:.4f}\")\n",
    "    print(f\"[Epoch {epoch}] Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"[Epoch {epoch}] Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    \n",
    "    step = _train_iteration * (epoch + 1)\n",
    "\n",
    "    writer.add_scalar('losses/train_loss', train_loss, step)\n",
    "    writer.add_scalar('losses/valid_loss', val_loss, step)\n",
    "    writer.add_scalar('losses/test_loss', test_loss, step)\n",
    "\n",
    "    writer.add_scalar('accuracy/train_acc', train_acc, step)\n",
    "    writer.add_scalar('accuracy/val_acc', val_acc, step)\n",
    "    writer.add_scalar('accuracy/test_acc', test_acc, step)\n",
    "\n",
    "    logger.append([train_loss, train_loss_x, train_loss_u, val_loss, val_acc, test_loss, test_acc])\n",
    "\n",
    "    is_best = val_acc > best_acc\n",
    "    best_acc = max(val_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'acc': val_acc,\n",
    "            'best_acc': best_acc,\n",
    "            'optimizer' : optim.state_dict(),\n",
    "        }, is_best)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "logger.close()\n",
    "writer.close()\n",
    "\n",
    "print('Best acc:')\n",
    "print(best_acc)\n",
    "\n",
    "print('Mean acc:')\n",
    "print(np.mean(test_accs[-20:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb67fc-c769-44de-ae04-3e94b66ffbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
